{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 130px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "\n",
    "<a href=\"https://openclassrooms.com/\" ><img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0d/Logo_OpenClassrooms.png\" style=\"float:right; max-width: 80px; display: inline\" alt=\"open Classrooms\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Apprentissage Machine: Valoriser les Données d'Objets Connectés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: ce tutoriel est utilisé dans ce MOOC comme <FONT COLOR=\"Red\"> *Fil Rouge*</font>  .  La bonne compréhension de la démarche mise en oeuvre ainsi celle des résultats obtenus est essentielle pour  la réussitre  de ce MOOC. \n",
    "\n",
    "Le tutoriel est découpée en trois parties successives correspondant aux trois parties du cours:\n",
    "<FONT COLOR=\"Red\">\n",
    "1. Exploration\n",
    "2. Apprentissage sur les données transformées\n",
    "3. Apprentissage profond sur les signaux bruts\n",
    "</font>\n",
    "\n",
    "Prenez le temps, au cours de l'exécution, de vous poser les questions marquées **Q**. Revenir si nécessaire au support de cours pour y répondre. \n",
    "\n",
    "Ce tutoriel va à l'essentiel pour atteindre l'objectif visé. Il peut se limiter à deux méthodes d'apprentissage: régression logistique et apprentissage profond. D'autres sont néanmoins utilisées à titre de comparaison. Pour compléter votre formation par une *farandole* des algoritmes d'apprentissage, réaliser jusqu'au bout le tutoriel de *rappel de statistique élémentaire* appliqué à la prévision de la concentration en ozone. Il introduit la plupart des méthodes d'apprentissage automatique. D'autres cas d'usage et tutoriels d'apprentissage automatique sont disponibles sur le dépôt [github.com/wikistat](https://github.com/wikistat/Apprentissage), notamment des exploitaitons plus complètes et détaillées de ce même jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas d'usage: reconnaissance de l'activité du porteur d'un smartphone\n",
    "# Apprentissage automatique en  <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a>  de signaux  temporels\n",
    "\n",
    "\n",
    "**Résumé**\n",
    "Cas d'usage de [reconnaissance d'activités humaines](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) à partir des enregistrements de signaux (gyroscope, accéléromètre) issus d'un *objet connecté*: un simple smartphone. Les données sont analysées pour illustrer les principales étapes communes en *science des données* et appliquables à des signaux physiques échantillonnés. Visualisation des signaux bruts afin d'évaluer les difficultés posées par ce type de données; exploration ([analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf), [analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf)) des données transformées (*features*) ou *métier* calculées à partir des signaux; prévision de l'activité à partir des données métier par des méthodes linéaires ([régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf)); prévision de l'activité à partir des signaux brutes par [réseau de neurones](http://wikistat.fr/pdf/st-m-app-rn.pdf) élémentaire puis [réseau convolutionnel](http://wikistat.fr/pdf/st-m-app-rn.pdf) (*deep learning*). Ce calepin montre les très bonnes qualités (96%) de prévision des méthodes élémentaires (linéaires) sur les données métier puis, pour économiser les coûteuses (pour la batterie embarquée) transformations, les mêmes qualités de prévision  sont obtenues par un réseau convolutionnel sur les signaux bruts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Introduction\n",
    "### Objectif général\n",
    "L'objectif est de reconnaître l'activité d'un individu porteur d'un smartphone qui enregistre un ensemble de signaux issu du gyroscope et de l'accéléromètre embarqués et ainsi connectés. Une base de données d'apprentissage a été construite expérimentalement. Un ensemble de porteurs d'un smartphone ont produit une activité déterminée pendant un laps de temps prédéfini tandis que des signaux étaient enregistrés. Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*). Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  L'analyse des données associée à une identification d'activité en temps réel, ne sont pas abordées.\n",
    "\n",
    "Les données publiques disponibles ont été acquises, décrites et analysées par [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf). Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine.\n",
    "\n",
    "L'archive contient les données brutes: accélérations échantillonnnées à 64 htz pendant 2s. Les accélérations en x, y, et z, chacune de 128 colonnes, celles en y soustrayant la gravité naturelle ainsi que les accélérations angulaires (gyroscope) en x, y, et z soit en tout 9 fichiers. Le choix d'une puissance de 2 pour la fréquence d'échantillonnage permet l'exécution efficace d'algorithmes de transformée de Fourier ou en ondelettes.\n",
    "\n",
    "\n",
    "### Déroulement\n",
    "Une première visualisation et exploration des signaux bruts montre (section 2) que ceux-ci sont difficiles à analyser; les classes d'activité y sont en effet mal caractérisées. La principale cause est l'absence de synchronisaiton des débuts d'activité; le déphasage des signaux apparaît alors comme un bruit ou artefact très préjudiciable à la bonne discrimination des activités sur la base d'une distance euclidienne usuelle ($L_2$). C'est la raison pour laquelle, [Anguita et al. (2012)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf) proposent de calculer un ensemble de transformations ou caractéristiques (*features*) des signaux: variance, corrélations, entropie, décompositions de Fourier... Ce sont alors $p=561$ variables qui sont considérées et explorées dans la section 3. L'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) et surtout l'[analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf) montrent les bonnes qulaités discriminatoires de ces données \"métier\" issues d'une connaissance experte des signaux. La section 4 exploite ces variables métier et montre que des modèles statistiques élémentaires car linéaires (régression logistique, analyse discriminante) ou qu'un algorithme classique de machine à vecteur support (SVM) utilisant un simple noyau linéaire conduit à d'excellentes prévisions au contraire d'algorihtmes non linaires sophistiqués (*random forest, gradient boosting*).\n",
    "\n",
    "Néanmoins, faire calculer en permanence des transformations sophistiquées (Fourier) n'est pas une solution viable pour la batterie d'un objet embarqué connecté. L'algorithme candidat doit pouvoir produire une solution intégrable (cablée) dans un cicuit, comme c'est par exemple le cas des puces dédiées à la reconnaissance faciale. C'est l'objet de la section 5: montrer la faisabilité d'une solution basée sur les seuls signaux brutes; solution mettant en oeuvre un réseau de neurones intégrant une [couche convolutionnelle](http://wikistat.fr/pdf/st-m-app-rn.pdf). \n",
    "\n",
    "### Environnement logiciel\n",
    "Pour être exécuté, ce calepin (*jupyter notebook*) nécessite l'utilisaiton de *Google Colab* ou l'installation de Python3 via par exemple le site  [Anaconda](https://conda.io/docs/user-guide/install/download.html). Les algorihtmes d'exploration et d'apprentissage statistiques utilisés sont disponibles dans la librairie [`Scikit-learn`](http://scikit-learn.org/stable/) tandis qu'une approche élémentaire de l'apprentissage profond des réseaux de neurones avec couche convolutionnelle nécessite l'installation de la librairie [`Keras`](https://keras.io/) qui entraine celle de [`TensorFlow`](https://www.tensorflow.org/). \n",
    "\n",
    "**Remarques**: \n",
    "- ce calepin a été construit et testé sous Ubuntu Mate 16.04 (Python 3.6) mais son utilisation sous Windows ou Mac OS ne devrait pas poser de problème une fois l'environnement correctement installé;\n",
    "- la commande `conda` installe sans difficulté l'environnement `Keras` en incluant `TensorFlow`;\n",
    "- la disponibilité d'une carte GPU est bienvenue pour réduire les temps d'apprentissage des réseaux épais!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\"> Partie 1 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etude préalable des signaux bruts\n",
    "### Source\n",
    "\n",
    "Les données sont celles originales du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle doivent être préalablement téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Chaque enregistrement ou unité statistique ou instance est labellisée avec **6 activités**: debout, assis, couché, marche, monter ou descendre un escalier. Chaque jeu de données est partagé en une partie échantillon d'apprentissage et une partie échantillon test. L'échantillon test n'est utilisé que pour évaluer et comparer les qualités de prévision des principales méthodes. Il est conservé en l'état afin de rendre les comparaisons possibles avec les résultats de la littérature. Il s'agit donc d'un problème de *classification supervisée* (6 classes) avec $n=10299$ échantillons pour l'apprentissage, 2947 pour le test.\n",
    "\n",
    "Les données contiennent deux jeux de dimensions différentes:\n",
    "\n",
    "1. Jeu multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(n, 128, 9)$\n",
    "2. Jeu unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(n, 1152)$\n",
    "\n",
    "*N.B.* La structure des données est nettement plus complexe que celles couramment étudiées dans le [dépôt Wikistat](https://github.com/wikistat/). Le code a été structuré en une séquence de fonctions afin d'en faciliter la compréhension. L'outil *calepin* atteint ici des limites pour la réalisation de codes complexes.\n",
    "\n",
    "\n",
    "### Importation des principales librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structurer les données\n",
    "Définir le chemin d'accès aux données puis les fonctions utiles.\n",
    "\n",
    "**Attention** le chemin d'accès aux données doit être adapté au contexte. L'accès distant fonctionne dans tous les cas mais prend plus de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention à la définition du chemin ci-dessous qui doit être adapté \n",
    "DATADIR_UCI = '~/Documents/2-Data_files/HAR/UCI HAR Dataset'\n",
    "# Liste des noms des fichiers afin d'automatiser la lecture.\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "# Fonctions permettant de lire la séquence des fichiers avant de restructurer les données \n",
    "# dans le fortmat recherché.\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = data_dir+'/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
    "    x = my_read_csv(filename).as_matrix()\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))    \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = data_dir+'/'+subset+'/y_'+subset+'.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).as_matrix()\n",
    "    else:\n",
    "        Y = y.as_matrix()\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions afin de s'assurer de la bonne lecture des fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "Certre phase est essentielle à la bonne compréhension des données, de leur structure et donc des problèmes qui vont être soulevés par la suite. La visualisation est très élémentaire d'un point de vue méthodologique mais nécessite des compétences plus éléborées en Python et donc des \n",
    "#### Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des couleurs\n",
    "CMAP = plt.get_cmap(\"Accent\")\n",
    "# Liste des types de signaux\n",
    "SIGNALS = [\"body_acc x\", \"body_acc y\", \"body_acc z\", \n",
    "                \"body_gyro x\", \"body_gyro y\", \"body_gyro z\", \n",
    "               \"total_acc x\", \"total_acc y\", \"total_acc z\"] \n",
    "# Dictionnaire en clair des activités expérimentées (contexte supervisé)\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "\n",
    "# Fonction pour le tracé d'un signal\n",
    "def plot_one_axe(X, fig, ax, sample_to_plot, cmap):\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            ax.plot(x, linewidth=1, color=cmap(act-1))\n",
    "def plot_one_axe_shuffle(X, fig, ax, sample_to_plot, cmap):\n",
    "    plot_data = []\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            plot_data.append([x,cmap(act-1)])\n",
    "    random.shuffle(plot_data)\n",
    "    for x,color in plot_data:\n",
    "        ax.plot(x, linewidth=1, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tracés de tous les signaux\n",
    "Tous les signaux sont tracés par type en superposant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_plot = 50\n",
    "index_per_act = [list(zip(np.repeat(act, sample_to_plot), np.where(Y_train_label==act)[0][:sample_to_plot])) for act in range(1,7)]\n",
    "index_to_plot = list(itertools.chain.from_iterable(index_per_act))\n",
    "random.shuffle(index_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for isignal in range(9):\n",
    "    ax = fig.add_subplot(3,3,isignal+1)\n",
    "    for act , i in index_to_plot:\n",
    "        ax.plot(range(128), X_train[i,:,isignal],color=CMAP(act-1), linewidth=1)\n",
    "        ax.set_title(SIGNALS[isignal])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Apprécier la difficulté à distinguer les activités au sein d'un même signal.\n",
    "\n",
    "### Par signal \n",
    "Le seul signal \"acélération en\" x est tracé en distinguant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_plot = 10\n",
    "isignal = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train_label==act)[0][:sample_to_plot]) for act in range(1,7)])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8), num=SIGNALS[isignal])\n",
    "for act , index in index_per_act_dict.items():\n",
    "    ax = fig.add_subplot(3,2,act)\n",
    "    for x in X_train[index]:\n",
    "        ax.plot(range(128), x[:,0],color=CMAP(act-1), linewidth=1)\n",
    "    ax.set_title(ACTIVITY_DIC[act])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelles est l'activité qui semble se distinguer facilement des autres? \n",
    "\n",
    "**Q** Observer les signaux d'une activité, par exemple `Walking upstairs`. Qu'est ce qui fait que, pour ces signaux ou courbes, une métrique euclidienne classique ($L_2$) est inopérante? \n",
    "\n",
    "**Q** Corrélativement pourquoi est-il important de décomposer un signal dans le domaine des fréquences?\n",
    "\n",
    "Un [autre calepin](https://github.com/wikistat/Exploration/blob/master/HumanActivityRecognition/Explo-Python-Har-brutes.ipynb) détaille plus avant l'exploration de ces données en proposant d'en faire une analyse en composantes principales et une analyse factorielle discriminate. Mais, comme ces dernières ne font que confirmer le faible pouvoir discriminant des signaux bruts au sens de la distance euclidienne classique, ils ne sont pas reproduits. En revanche, l'exploration des données métier ci-dessous est approfondie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration des données métier\n",
    "### Les données\n",
    "L'[archive de l'UCI]() contient également deux fichiers `train` et `test` des 561 caractéristiques (*features*) ou variables \"métier\" calculées dans les domaines temporels et fréquentiels par transformation des signaux bruts.\n",
    "\n",
    "Voici une liste indicative des variables calculées sur chacun des signaux bruts ou couples de signaux:\n",
    "\n",
    "Name|Signification\n",
    "-|-\n",
    "mean | Mean value\n",
    "std | Standard deviation\n",
    "mad | Median absolute value\n",
    "max | Largest values in array\n",
    "min | Smallest value in array\n",
    "sma | Signal magnitude area\n",
    "energy | Average sum of the squares\n",
    "iqr | Interquartile range\n",
    "entropy | Signal Entropy\n",
    "arCoeff | Autorregresion coefficients\n",
    "correlation | Correlation coefficient\n",
    "maxFreqInd | Largest frequency component\n",
    "meanFreq | Frequency signal weighted average\n",
    "skewness | Frequency signal Skewness\n",
    "kurtosis | Frequency signal Kurtosis\n",
    "energyBand | Energy of a frequency interval\n",
    "angle | Angle between two vectors\n",
    "\n",
    "#### Lecture des données métier\n",
    "Moins volumineuses, elles ont été chargées dans le dépôt simultanément à ce calepin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des données d'apprentissage\n",
    "# Attention, il peut y avoir plusieurs espaces comme séparateur dans le fichier\n",
    "Xtrain=pd.read_table(\"X_train.txt\",sep='\\s+',header=None)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "ytrain=pd.read_table(\"y_train.txt\",sep='\\s+',header=None,names=('y'))\n",
    "# Le type dataFrame est inutile et même gênant pour les la suite\n",
    "ytrain=ytrain[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des données de test\n",
    "Xtest=pd.read_table(\"X_test.txt\",sep='\\s+',header=None)\n",
    "Xtest.shape\n",
    "ytest=pd.read_table(\"y_test.txt\",sep='\\s+',header=None,names=('y'))\n",
    "ytest=ytest[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "#### Principe\n",
    "Il est important de se faire une idée précise de la structure des données.  Une analyse en composantes principales est adaptée à cet objectif. \n",
    "\n",
    "   - Elle recherche les axes de plus grande dispersion du nuages des individus dans $R^p$ avec $p=561$. Ces axes sont définis par les vecteurs propres de la matrice des covariances ou des corrélations si les variables sont réduites (divisées par l'écart-type).\n",
    "   - Les représentations graphiques des individus sont obtenues par projection sur les sous-espaces engendrés par les premiers vecteurs propres. Elles préservent au mieux les distances entre ceux-ci.\n",
    "   - Les coordonnées sont stockées dans la matrice des *composantes principales* qui sont aussi les combinaisons linéaires de plus grande variance des variables. Ce sont des variables décorrélées, orthogonales deux à deux.\n",
    "   - Les representations graphiques des variables initiales conservent au mieux les angles entre les vecteurs variables dans l'espace $R^n$ de façon à interpréter leurs corrélations qui, géométriquement, sont les cosinus de ces angles. \n",
    "\n",
    "La fonction définie ci-après affiche un nuage de points dans un plan factoriel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X_R,fig,ax,nbc,nbc2):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = ACTIVITY_DIC [i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=1, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=10)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice des composantes principales. C'est aussi un changement (transformation) de base; de la base canonique dans la base des vecteurs propres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "pca = PCA()\n",
    "X_r = pca.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valeurs propres ou variances des composantes principales\n",
    "Représentation de la décroissance des valeurs propres, les variances des variables ou composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un graphique plus explicite décrit les distribution de ces composantes par des diagrames boîtes; seules les premières sont affichées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(X_r[:,0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenter la décroissance des variances, le choix éventuel d'une dimension ou nombre de composantes à retenir sur les 561.\n",
    "#### Représentation des individus ou \"activités\" en ACP\n",
    "Projection dans les principaux plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaps = plt.get_cmap(\"Accent\")\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_r, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Commenter la séparation des deux types de situation par le premier axe.\n",
    "\n",
    "**Q** Que dire sur la forme des nuages?\n",
    "\n",
    "**Q** Que dire sur la plus ou moins bonne séparation des classes?\n",
    "#### Représentation des variables en ACP\n",
    "Lecture des libellés des variables et constitution d'une liste. Souci de la grande dimension (561), les représentations ne sont guère exploitables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.txt', 'r') as content_file:\n",
    "    featuresNames = content_file.read()\n",
    "columnsNames = list(map(lambda x : x.split(\" \")[1],featuresNames.split(\"\\n\")[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphe des variables illisible en mettant les libellés en clair. Seule une * est représentée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordonnées des variables\n",
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j in zip(coord1,coord2, ):\n",
    "    plt.text(i, j, \"*\")\n",
    "    plt.arrow(0,0,i,j,color='r')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='b', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identification des variables participant le plus au premier axe. Ce n'est pas plus clair! Seule la réprésentation des individus apporte finalement des éléments de compréhension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(columnsNames)[abs(coord1)>.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Analyse Factorielle Discriminante (AFD)](http://wikistat.fr/pdf/st-m-explo-afd.pdf)\n",
    "#### Principe\n",
    "L'ACP ne prend pas en compte la présence de la variable qualitative à modéliser contrairement à l'analyse factorielle discriminante (AFD) adaptés à ce contexte \"supervisé\" puisque l'activité est connue sur un échantillon d'apprentissage. L'AFD est une ACP des barycentres des classes munissant l'espace des individus d'une métrique spécifique dite de *Mahalanobis*. Métrique définie par l'inverse de la matrice de covariance intraclase. L'objectif est alors de visualiser les capacités des variables à discriminer les classes.\n",
    "\n",
    "La librairie `scikit-learn` ne propose pas de fonction spécifique d'analyse factorielle discriminante mais les coordonnées des individus dans la base des vecteurs discriminants sont obtenues comme résultats de l'analyse discriminante linéaire décisionnnelle. Cette dernière sera utilisé avec une finalité prédictive dans un deuxième temps (autre calepin). \n",
    "\n",
    "Les résultats de la fonction `LinearDiscriminantAnalysis` de `scikit-learn` sont identiques à ceux de la fonction `lda` de R. Elle eest donc utilisée strictement de la même façon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "method = LinearDiscriminantAnalysis() \n",
    "lda=method.fit(Xtrain,ytrain)\n",
    "X_r2=lda.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que signifie le *warning*? Quel traitement faudrait-t-il mettre en oeuvre pour utiliser une analyse discriminante décisionnelle en modélisation ou apprentissage.\n",
    "\n",
    "#### Représentation des individus en AFD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_r2, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la séparation des classes. Sont-elles toutes séparables deux à deux?\n",
    "\n",
    "**Q** Que dire de la forme des nuages notamment dans le premier plan?\n",
    "\n",
    "Comme pour l'ACP, la représentation trop complexe des variables n'apporterait rien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\"> Partie 2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prévision de l'activité à partir des variables métier\n",
    "D'autres méthodes sont successivement testées dans les calepins complétant l'étude: SVM, analyse discriminate décisionnelle, $k$ plus proches voisins, forêts aléatoires, réseaux de neurones... Seule la régression logistique est utilisée dans ce calepin pour illustrer la phase d'apprentissage / modélisation pour la prévision du comportement.\n",
    "\n",
    "### [Régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "\n",
    "####  Principe\n",
    "Une méthode statistique ancienne mais finalement efficace sur ces données. La régression logistique est adaptée à la prévision d'une variable binaire. Dans le cas multiclasse, la fonction logistique de la librairie `Scikit-learn` estime *par défaut* **un modèle par classe**: une classe contre les autres. \n",
    "\n",
    "La probabilité d'appartenance d'un individu à une classe est modélisée à l'aide d'une combinaison linéaire des variables explicatives. Pour transformer une combinaison linéaire à valeur dans $R$ en une probabilité à valeurs dans l'intervalle $[0, 1]$, une fonction de forme sigmoïdale est appliquée.  Ceci donne: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ ou, c'est équivalent, une décomposition linéaire du *logit* ou *log odd ratio* de  $P(y_i=1)$:  $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$\n",
    "\n",
    "\n",
    "####  Estimation du modèle sans optimisation\n",
    "Le modèle est estimé sans chercher à raffiner les valeurs de certains paramètres (pénalisation). Ce sera fait dans un deuxième temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "ts = time.time()\n",
    "method = LogisticRegression()\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prévision de l'activité de l'échantillon test\n",
    "Une fois le modèle estimé, l'erreur de prévision est évaluée, sans biais optimiste, sur un autre échantillon, dit échantillon test, qui n'a pas participé à l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "labels = ACTIVITY_DIC.values()\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelles sont les classes qui restent difficiles à discriminer?\n",
    "\n",
    "**Q** Commenter la qualité des résultats obtenus. Sont-ils cohérents avec l'approche exploratoire.\n",
    "\n",
    "#### Optimisation du modèle par pénalisation Lasso\n",
    "***Attention* l'exécution est un peu longue... **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[3,4,5]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\"), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(Xtrain, ytrain)  \n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(Xtest)\n",
    "# matrice de confusion\n",
    "logitOpt.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitOpt.best_params_['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** L'amélioration obtenue par la séleciton de variable est-elle bien significative?\n",
    "\n",
    "Les coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "method = LogisticRegression(C=logitOpt.best_params_['C'])\n",
    "model_lasso=method.fit(Xtrain,ytrain)\n",
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un modèle par classe et donc une matrice de coefficients. seul le modèle associée à la première classe est considéré à titre d'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(model_lasso.coef_[0], index = Xtrain.columns)\n",
    "print(\"Lasso conserve \" + str(sum(coef != 0)) + \n",
    "      \" variables et en supprime \" +  str(sum(coef == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la sélection et donc du nombre de variables retenus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [*Random forest*](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "**Cette sous-section peut être sautée dans le cadre du MOOC**\n",
    "\n",
    "**Q** Quel serait le paramètre à optimiser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ts = time.time()\n",
    "method = RandomForestClassifier(n_estimators=200,n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVM linéaire](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "** Cette sous section peut être sautée dans le cadre du MOOC**\n",
    "\n",
    "**Q** Est-il utile d'optimiser le paramètre de pénalisation dans le cas linéaire? Pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "ts = time.time()\n",
    "method = LinearSVC()\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinaison de modèles\n",
    "Les formes des nuages de chaque classe observées dans le premier plan de l'analyse en composantes principales montrent que la structure de covariance n'est pas identique dans chaque classe. Cette remarque suggèrerait de s'intéresser à l'analyse discriminante quadratique mais celle-ci bloque sur l'estimation six matrices de covariance et de leurs inverses. Néanmoins il semble que, plus précisément, deux groupes se distinguent: les classes actives (marcher, monter ou descendre un escalier) d'une part et les classes passives (couché, assis, debout) d'autre part et, qu'à l'intérieur de chaque groupe les variances sont assez similaires. \n",
    "\n",
    "Cette situation suggère de construire une décision en deux étapes ou hiérarchique:\n",
    "    1. Régression logistique séparant les activités passives *vs.* actives,\n",
    "    2. Un modèle spécifique à chacune des classes précédentes, par exemple des SVM à noyau gaussien.\n",
    "\n",
    "Une telle construction hiérarchique de modèles aboutit à une précision supérieure à 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\"> Partie 3 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prévision de l'activité à partir des signaux bruts\n",
    "### Introduction\n",
    "Comme expliqué en introduction, le calcul des nombreuses transformations des données est bien trop consommateur des ressources de la batterie d'un objet connecté. Cette section se propose d'utiliser les seules signaux bruts pour faire apprendre un algorithme et parmi ceux-ci seuls les réseaux de neurones pouvant être \"cablés\" dans un circuit sont pris en compte. En effet un algorithme de type XGBoost (extrem gradient boosting) parvient également à des  résultats raisonables sur les signaux mais à un coût trop élevé. \n",
    "\n",
    "Deux algorithmes sont successivement testés: un rseau de type perceptron classique suivi d'un réseau avec couche de convoluyion 1D sur les signaux. Enfin, il se trouve que, combiner ces deux algoritnmes, par la simple moyene des prévision des probabilités d'identification d'activité conduit à un bon résultat. \n",
    "\n",
    "Il faudrait ajouter que de nombreuses configurations on été testées (merci aux étudiants de l'[INSA de Toulouse de la spécialité Mathématiques Appliquées - Science des Données](http://www.math.insa-toulouse.fr/fr/index.html)): LSTM, convolution 2D... avant de converger vers celle proposée. C'est une réalité, de l'apprentissage épais, seule une approche très heuristique des solutions possibles dans l'empilement des couches successives, permet de détrerminer la solution la plus performantes. \n",
    "\n",
    "### Perceptron à une couche cachée\n",
    "#### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.models as km \n",
    "import keras.layers as kl \n",
    "import keras.layers.core as klc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "def my_confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition du réseau \n",
    "Une couche cachée, une couche de reformattage puis une couche de sortie à 6 classes. Le nombre de neurones (50) sur la couche cachée a été optimisé par ailleurs. Le nombre d'epochs reste raisonnable et la taille des batchs devraient être optimisés surtout dans le cas d'utilisation d'une carte GPU.\n",
    "\n",
    "Remarquer le nombre de paramètres à estimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "n_hidden = 50\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "model_base_mlp =km.Sequential()\n",
    "model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"\\nScore With Simple MLP on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_prediction = model_base_mlp.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau avec couche convolutionnelle\n",
    "L'idée pertinente avec ces données est évidemment d'identifier le problème lié au déphasage des signaux. L'utilisation d'une couche convolutionnelle introduit une propriété d'invariabce par translation. Les caractéristiques ou *features* sortant de cette couche acquièrent donc ainsi de bonnes propriétés avant d'être dirigées vers des couches techniques intermédiaires (`MaxPooling, Flatten`) et une dernière couche de sortie qui effectue la discrimination à partir des caractéristiques.\n",
    "\n",
    "Remarquer le nombre de paramètres à estimer, le comparer avec celui du perceptron précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = len(X_train[0])\n",
    "batch_size= 32\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "epochs=20\n",
    "\n",
    "model_base_conv_1D =km.Sequential()\n",
    "model_base_conv_1D.add(kl.Conv1D(32, 9, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "model_base_conv_1D.add(kl.MaxPooling1D(pool_size=3))\n",
    "model_base_conv_1D.add(kl.Flatten())\n",
    "model_base_conv_1D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "model_base_conv_1D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_conv_1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_conv_1D.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_conv_1D.evaluate(X_test, Y_test_dummies)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_conv = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_conv_1D_prediction = model_base_conv_1D.predict(X_test)\n",
    "my_confusion_matrix(Y_test_dummies, base_conv_1D_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score With Conv on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques efforts supplémentaires permettraient sans doute de gratter quelques points sur la précision du résultat mais... attention au sur-apprentissage si le même échantillon test est toujours utilisé. La mise en oeuvre pertinente d'algorithmes d'apprentissage épais nécessite de tester empiriquement beaucoup de solutions; introduire des modèles plus complexes, avec plus de couches pour améliorer encore la qualité de la discrimination des données autrement plus volumineuses. \n",
    "\n",
    "L'objectif final de ce calepin n'est pas de trouver la meilleure solution mais bien de montrer qu'un traitement des signaux bruts avec les algorithmes adaptés conduit à des résultats similaires à ceux obtenus par l'analyse des données issues de l'expertise de spécialistes du traitement du signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Références**\n",
    "\n",
    "D. Anguita, A. Ghio, L. Oneto, X. Parra, J.L. (2012). [Human Activity Recognition on Smartphones Using a Multiclass Hardware-friendly Support Vector Machine](https://www.icephd.org/sites/default/files/IWAAL2012.pdf), *Proceedings of the 4th International Conference on Ambient Assisted Living and Home Care*, IWAAL'1, 216- 223, Springer-Verlag.\n",
    "\n",
    "D. Dua, E. Karra Taniskidou (2017). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
